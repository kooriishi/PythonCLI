#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæœ¬åœ°é‡åŒ–æ¨¡å‹æ¨ç† CLI å·¥å…·
åŸºäº llama-cpp-python çš„ Qwen1.5-0.5B å¯¹è¯ç³»ç»Ÿ

æ¨¡å—åŒ–æ¶æ„ï¼š
- performance_monitor.py: æ€§èƒ½ç›‘æ§æ¨¡å—
- hardware_detector.py: ç¡¬ä»¶æ£€æµ‹æ¨¡å—
- llm_core.py: LLMæ ¸å¿ƒæ¨ç†æ¨¡å—
- concurrent_handler.py: å¹¶å‘å¤„ç†æ¨¡å—
- cli_interface.py: CLIäº¤äº’ç•Œé¢æ¨¡å—
- main.py: ä¸»ç¨‹åºå…¥å£
"""

import os
import sys
sys.path.append("./modules")
import argparse

# å¯¼å…¥æ¨¡å—
try:
    import colorama
    from colorama import Fore, Style
    import psutil
    
    # å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
    from hardware_detector import HardwareDetector
    from performance_monitor import PerformanceMonitor
    from cli_interface import ChatCLI, SimplePromptInterface
    
except ImportError as e:
    print(f"è¯·å®‰è£…ä¾èµ–: pip install llama-cpp-python colorama psutil aiohttp")
    print(f"ç¼ºå°‘æ¨¡å—: {e}")
    sys.exit(1)

# åˆå§‹åŒ–é¢œè‰²è¾“å‡º
colorama.init()


def print_welcome():
    """æ‰“å°æ¬¢è¿ä¿¡æ¯"""
    welcome = f"""
{Fore.CYAN}ğŸš€ å¢å¼ºç‰ˆæœ¬åœ°é‡åŒ–æ¨¡å‹æ¨ç†å·¥å…·{Style.RESET_ALL}
{Fore.YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{Style.RESET_ALL}
{Fore.GREEN}âœ¨ æ–°å¢åŠŸèƒ½:{Style.RESET_ALL}
  â€¢ ğŸ–¥ï¸  æ€§èƒ½ç›‘æ§ä»ªè¡¨ç›˜
  â€¢ ğŸ”§ å¤šç¡¬ä»¶åç«¯æ”¯æŒ (CPU/CUDA/OpenCL/Metal)
  â€¢ âš¡ å¹¶å‘è¯·æ±‚å¤„ç†
  â€¢ ğŸ“Š å®æ—¶æ€§èƒ½ç»Ÿè®¡
  â€¢ ğŸ¨ å¢å¼ºçš„ç”¨æˆ·ç•Œé¢

{Fore.BLUE}ğŸ“š æ¨¡å—åŒ–è®¾è®¡:{Style.RESET_ALL}
  â€¢ performance_monitor.py - æ€§èƒ½ç›‘æ§
  â€¢ hardware_detector.py - ç¡¬ä»¶æ£€æµ‹
  â€¢ llm_core.py - æ ¸å¿ƒæ¨ç†
  â€¢ concurrent_handler.py - å¹¶å‘å¤„ç†
  â€¢ cli_interface.py - ç”¨æˆ·ç•Œé¢
{Fore.YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{Style.RESET_ALL}
"""
    print(welcome)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆæœ¬åœ°é‡åŒ–æ¨¡å‹æ¨ç†å·¥å…·")
    parser.add_argument("--model", "-m", default="models/qwen1_5-0_5b-chat-q4_0.gguf",
                       help="æ¨¡å‹æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--backend", "-b", default="auto",
                       choices=["auto", "cpu", "cuda", "opencl", "metal"],
                       help="ç¡¬ä»¶åç«¯é€‰æ‹© (auto/cpu/cuda/opencl/metal)")
    parser.add_argument("--temperature", "-t", type=float, default=0.7,
                       help="æ¸©åº¦å‚æ•° (0.1-2.0)")
    parser.add_argument("--max-tokens", "-l", type=int, default=512,
                       help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--threads", "-j", type=int, default=0,
                       help="CPUçº¿ç¨‹æ•° (0=è‡ªåŠ¨)")
    parser.add_argument("--concurrent", "-c", type=int, default=1,
                       help="æœ€å¤§å¹¶å‘è¯·æ±‚æ•° (1-8)")
    parser.add_argument("--monitor", action="store_true",
                       help="å¯ç”¨æ€§èƒ½ç›‘æ§")
    parser.add_argument("--prompt", "-p", type=str,
                       help="å•æ¬¡é—®ç­”æ¨¡å¼")
    parser.add_argument("--list-backends", action="store_true",
                       help="æ˜¾ç¤ºå¯ç”¨ç¡¬ä»¶åç«¯")
    parser.add_argument("--system-info", action="store_true",
                       help="æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯")
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    if not args.prompt and not args.list_backends and not args.system_info:
        print_welcome()
    
    # å‚æ•°éªŒè¯
    if args.concurrent < 1 or args.concurrent > 8:
        print(f"{Fore.RED}âŒ å¹¶å‘æ•°å¿…é¡»åœ¨1-8ä¹‹é—´{Style.RESET_ALL}")
        sys.exit(1)
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    if args.system_info:
        HardwareDetector.print_system_info()
        return
    
    # æ˜¾ç¤ºå¯ç”¨åç«¯
    if args.list_backends:
        detector = HardwareDetector()
        backends = detector.detect_available_backends()
        print(f"\n{Fore.CYAN}ğŸ–¥ï¸ å¯ç”¨ç¡¬ä»¶åç«¯:{Style.RESET_ALL}")
        for backend, available in backends.items():
            status = f"{Fore.GREEN}âœ“{Style.RESET_ALL}" if available else f"{Fore.RED}âœ—{Style.RESET_ALL}"
            config = detector.get_optimal_config(backend)
            print(f"  {status} {backend.upper()}: {config}")
        print()
        return
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(args.model):
        print(f"{Fore.RED}âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶è·¯å¾„æ­£ç¡®{Style.RESET_ALL}")
        sys.exit(1)
    
    # è‡ªåŠ¨è®¾ç½®çº¿ç¨‹æ•°
    if args.threads == 0:
        args.threads = min(8, psutil.cpu_count())
    
    config = {
        'temperature': args.temperature,
        'max_tokens': args.max_tokens,
        'n_threads': args.threads
    }
    
    # å•æ¬¡é—®ç­”æ¨¡å¼
    if args.prompt:
        SimplePromptInterface.run_single_prompt(
            args.model, 
            args.prompt, 
            backend=args.backend,
            monitor=args.monitor,
            **config
        )
        return
    
    # äº¤äº’æ¨¡å¼
    cli = ChatCLI(enable_monitoring=args.monitor, max_concurrent=args.concurrent)
    cli.run(args.model, backend=args.backend, **config)


if __name__ == "__main__":
    main()