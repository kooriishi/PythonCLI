#!/usr/bin/env python3
"""
LLMæ ¸å¿ƒæ¨ç†æ¨¡å—
æä¾›æœ¬åœ°é‡åŒ–æ¨¡å‹æ¨ç†åŠŸèƒ½
"""

import os
import time
import threading
from datetime import datetime
from typing import Generator, List, Dict, Optional

try:
    import colorama
    from colorama import Fore, Style
    from llama_cpp import Llama
except ImportError as e:
    print(f"è¯·å®‰è£…ä¾èµ–: pip install llama-cpp-python colorama")
    raise e

from hardware_detector import HardwareDetector
from performance_monitor import PerformanceMonitor


class SimpleLLM:
    """ç®€åŒ–çš„æœ¬åœ° LLM æ¨ç†å™¨ï¼ˆæ”¯æŒå¤šç¡¬ä»¶åç«¯å’Œæ€§èƒ½ç›‘æ§ï¼‰"""
    
    def __init__(self, model_path: str, backend: str = 'auto', monitor: Optional[PerformanceMonitor] = None, **kwargs):
        self.model_path = model_path
        self.backend = backend
        self.monitor = monitor
        self.llm = None
        self.chat_history = []
        self.total_chars = 0
        self.total_time = 0.0
        self.start_time = datetime.now()
        self.lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨
        
        # æ£€æµ‹å¯ç”¨åç«¯
        self.available_backends = HardwareDetector.detect_available_backends()
        
        # é€‰æ‹©åç«¯
        if backend == 'auto':
            self.selected_backend = self._select_best_backend()
        else:
            self.selected_backend = backend if self.available_backends.get(backend, False) else 'cpu'
        
        # è·å–åç«¯ä¼˜åŒ–é…ç½®
        backend_config = HardwareDetector.get_optimal_config(self.selected_backend)
        
        # é»˜è®¤å‚æ•°ï¼Œç»“åˆåç«¯ä¼˜åŒ–
        self.config = {
            'n_ctx': kwargs.get('n_ctx', 2048),
            'n_threads': kwargs.get('n_threads', backend_config['n_threads']),
            'n_gpu_layers': kwargs.get('n_gpu_layers', backend_config.get('n_gpu_layers', 0)),
            'temperature': kwargs.get('temperature', 0.7),
            'top_p': kwargs.get('top_p', 0.9),
            'max_tokens': kwargs.get('max_tokens', 512),
            'verbose': kwargs.get('verbose', False)
        }
    
    def _select_best_backend(self) -> str:
        """è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯"""
        if self.available_backends.get('cuda', False):
            return 'cuda'
        elif self.available_backends.get('metal', False):
            return 'metal'
        elif self.available_backends.get('opencl', False):
            return 'opencl'
        else:
            return 'cpu'
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹"""
        print(f"{Fore.YELLOW}æ­£åœ¨åŠ è½½æ¨¡å‹: {os.path.basename(self.model_path)}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}ä½¿ç”¨åç«¯: {self.selected_backend.upper()}{Style.RESET_ALL}")
        
        if not os.path.exists(self.model_path):
            print(f"{Fore.RED}âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}{Style.RESET_ALL}")
            return False
        
        try:
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=self.config['n_ctx'],
                n_threads=self.config['n_threads'],
                n_gpu_layers=self.config['n_gpu_layers'],
                verbose=self.config['verbose']
            )
            print(f"{Fore.GREEN}âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!{Style.RESET_ALL}")
            
            # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
            print(f"{Fore.CYAN}é…ç½®ä¿¡æ¯:{Style.RESET_ALL}")
            print(f"  â€¢ ä¸Šä¸‹æ–‡é•¿åº¦: {self.config['n_ctx']}")
            print(f"  â€¢ CPUçº¿ç¨‹æ•°: {self.config['n_threads']}")
            print(f"  â€¢ GPUå±‚æ•°: {self.config['n_gpu_layers']}")
            
            return True
        except Exception as e:
            print(f"{Fore.RED}âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}{Style.RESET_ALL}")
            return False
    
    def generate_response(self, user_input: str) -> Generator[str, None, None]:
        """æµå¼ç”Ÿæˆå›å¤ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if not self.llm:
            yield "æ¨¡å‹æœªåŠ è½½"
            return
        
        # æ€§èƒ½ç›‘æ§
        if self.monitor:
            self.monitor.record_request_start()
        
        with self.lock:  # ä¿è¯çº¿ç¨‹å®‰å…¨
            # è®°å½•ç”¨æˆ·è¾“å…¥
            self.chat_history.append({"role": "user", "content": user_input})
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(user_input)
        
        try:
            start_time = time.time()
            response_text = ""
            token_count = 0
            
            # æµå¼ç”Ÿæˆ
            stream = self.llm(
                prompt,
                max_tokens=self.config['max_tokens'],
                temperature=self.config['temperature'],
                top_p=self.config['top_p'],
                repeat_penalty=1.1,
                stream=True,
                stop=["<|im_end|>", "<|im_start|>", "\n\n<|im_start|>"]
            )
            
            for output in stream:
                if 'choices' in output and len(output['choices']) > 0:
                    token = output['choices'][0]['text']
                    response_text += token
                    token_count += 1
                    yield token
            
            # è®°å½•ç»Ÿè®¡
            generation_time = time.time() - start_time
            self.total_chars += len(response_text)
            self.total_time += generation_time
            
            # æ€§èƒ½ç›‘æ§
            if self.monitor:
                self.monitor.record_request_end(generation_time, token_count)
            
            with self.lock:
                # è®°å½•åŠ©æ‰‹å›å¤
                clean_response = response_text.strip()
                self.chat_history.append({"role": "assistant", "content": clean_response})
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            chars_per_sec = len(response_text) / generation_time if generation_time > 0 else 0
            yield f"\n{Fore.CYAN}ğŸ“Š {len(response_text)} å­—ç¬¦ | {generation_time:.2f}s | {chars_per_sec:.1f} å­—ç¬¦/ç§’ | {token_count} tokens{Style.RESET_ALL}"
            
        except Exception as e:
            if self.monitor:
                self.monitor.record_request_end(0, 0)
            yield f"\n{Fore.RED}âŒ ç”Ÿæˆé”™è¯¯: {e}{Style.RESET_ALL}"
    
    def _build_prompt(self, user_input: str) -> str:
        """æ„å»ºå¯¹è¯æç¤ºè¯"""
        # ä½¿ç”¨æ›´é€‚åˆQwenæ¨¡å‹çš„å¯¹è¯æ ¼å¼
        conversation = ["<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡ç®€æ´åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚<|im_end|>"]
        
        # æ·»åŠ æœ€è¿‘çš„å¯¹è¯å†å² (æœ€å¤š6è½®)
        recent_history = self.chat_history[-6:]
        for msg in recent_history:
            if msg["role"] == "user":
                conversation.append(f"<|im_start|>user\n{msg['content']}<|im_end|>")
            else:
                conversation.append(f"<|im_start|>assistant\n{msg['content']}<|im_end|>")
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        conversation.append(f"<|im_start|>user\n{user_input}<|im_end|>")
        conversation.append("<|im_start|>assistant\n")
        
        return "\n".join(conversation)
    
    def clear_history(self):
        """æ¸…ç©ºèŠå¤©å†å²"""
        with self.lock:
            self.chat_history.clear()
        print(f"{Fore.GREEN}âœ… èŠå¤©å†å²å·²æ¸…ç©º{Style.RESET_ALL}")
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        uptime = datetime.now() - self.start_time
        user_msgs = sum(1 for msg in self.chat_history if msg["role"] == "user")
        ai_msgs = sum(1 for msg in self.chat_history if msg["role"] == "assistant")
        avg_speed = self.total_chars / self.total_time if self.total_time > 0 else 0
        
        return {
            "è¿è¡Œæ—¶é—´": str(uptime).split('.')[0],
            "å¯¹è¯è½®æ•°": user_msgs,
            "æ€»å­—ç¬¦æ•°": self.total_chars,
            "å¹³å‡é€Ÿåº¦": f"{avg_speed:.1f} å­—ç¬¦/ç§’",
            "æ¨¡å‹æ–‡ä»¶": os.path.basename(self.model_path),
            "åç«¯ç±»å‹": self.selected_backend.upper(),
            "å…è®¸åç«¯": ", ".join([k.upper() for k, v in self.available_backends.items() if v])
        }