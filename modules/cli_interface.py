#!/usr/bin/env python3
"""
CLIäº¤äº’ç•Œé¢æ¨¡å—
æä¾›å‘½ä»¤è¡Œç”¨æˆ·ç•Œé¢å’Œäº¤äº’åŠŸèƒ½
"""

import os
from typing import Dict, Optional

try:
    import colorama
    from colorama import Fore, Style
except ImportError as e:
    print(f"è¯·å®‰è£…ä¾èµ–: pip install colorama")
    raise e

from llm_core import SimpleLLM
from concurrent_handler import ConcurrentLLM
from performance_monitor import PerformanceMonitor
from hardware_detector import HardwareDetector


class ChatCLI:
    """å¢å¼ºçš„èŠå¤© CLIï¼ˆæ”¯æŒæ€§èƒ½ç›‘æ§å’Œå¹¶å‘å¤„ç†ï¼‰"""
    
    def __init__(self, enable_monitoring: bool = False, max_concurrent: int = 1):
        self.llm = None
        self.monitor = PerformanceMonitor() if enable_monitoring else None
        self.max_concurrent = max_concurrent
        self.enable_monitoring = enable_monitoring
        self.monitor_display_active = False
    
    def print_banner(self):
        """æ‰“å°æ¬¢è¿ä¿¡æ¯"""
        banner = f"""
{Fore.CYAN}â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                ğŸ¤– å‡çº§ç‰ˆæœ¬åœ°é‡åŒ–æ¨¡å‹æ¨ç†å·¥å…·                â”‚
â”‚              åŸºäº llama-cpp-python + Qwen1.5-0.5B            â”‚
â”‚                                                              â”‚
â”‚  æ–°å¢åŠŸèƒ½: æ€§èƒ½ç›‘æ§ | å¤šç¡¬ä»¶åç«¯ | å¹¶å‘å¤„ç†               â”‚
â”‚  æœ¬åœ°æ¨ç† | æµå¼è¾“å‡º | å¯¹è¯å†å² | æ€§èƒ½ç»Ÿè®¡               â”‚
â”‚  å‘½ä»¤: /help /clear /stats /monitor /backends /quit         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯{Style.RESET_ALL}
"""
        print(banner)
    
    def run(self, model_path: str, backend: str = 'auto', **config):
        """è¿è¡ŒèŠå¤©ç¨‹åº"""
        # åˆå§‹åŒ–æ¨¡å‹
        if self.max_concurrent > 1:
            self.llm = ConcurrentLLM(model_path, self.max_concurrent, self.monitor, backend=backend, **config)
        else:
            self.llm = SimpleLLM(model_path, backend=backend, monitor=self.monitor, **config)
        
        if not self.llm.load_model():
            return
        
        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        if self.monitor:
            self.monitor.start_monitoring()
            print(f"{Fore.GREEN}ğŸ–¥ï¸ æ€§èƒ½ç›‘æ§å·²å¯åŠ¨ (è¾“å…¥ /monitor æŸ¥çœ‹){Style.RESET_ALL}")
        
        self.print_banner()
        print(f"{Fore.GREEN}ğŸ’¬ å¼€å§‹å¯¹è¯! (è¾“å…¥ /help æŸ¥çœ‹å‘½ä»¤){Style.RESET_ALL}")
        if self.max_concurrent > 1:
            print(f"{Fore.CYAN}ğŸš€ å¹¶å‘æ¨¡å¼å·²å¯ç”¨ï¼Œæœ€å¤§å¹¶å‘æ•°: {self.max_concurrent}{Style.RESET_ALL}")
        print("=" * 60)
        
        # ä¸»èŠå¤©å¾ªç¯
        try:
            while True:
                try:
                    # è·å–ç”¨æˆ·è¾“å…¥
                    user_input = input(f"\n{Fore.BLUE}ä½ : {Style.RESET_ALL}").strip()
                    
                    if not user_input:
                        continue
                    
                    # å¤„ç†å‘½ä»¤
                    if user_input.startswith('/'):
                        if self.handle_command(user_input):
                            break
                        continue
                    
                    # ç”Ÿæˆå›å¤
                    print(f"{Fore.GREEN}åŠ©æ‰‹: {Style.RESET_ALL}", end="", flush=True)
                    
                    for token in self.llm.generate_response(user_input):
                        print(token, end="", flush=True)
                    print()  # æ¢è¡Œ
                    
                except KeyboardInterrupt:
                    print(f"\n\n{Fore.YELLOW}ğŸ‘‹ å†è§!{Style.RESET_ALL}")
                    break
                except Exception as e:
                    print(f"\n{Fore.RED}âŒ é”™è¯¯: {e}{Style.RESET_ALL}")
        finally:
            # æ¸…ç†èµ„æº
            if self.monitor:
                self.monitor.stop_monitoring()
            if hasattr(self.llm, 'shutdown'):
                self.llm.shutdown()
    
    def handle_command(self, command: str) -> bool:
        """å¤„ç†å‘½ä»¤ï¼Œè¿”å›Trueè¡¨ç¤ºé€€å‡º"""
        cmd = command.lower()
        
        if cmd in ['/quit', '/exit', '/q']:
            print(f"{Fore.YELLOW}ğŸ‘‹ å†è§!{Style.RESET_ALL}")
            return True
        
        elif cmd in ['/help', '/h']:
            self.show_help()
        
        elif cmd in ['/clear', '/c']:
            if self.llm:
                self.llm.clear_history()
            self._clear_screen()
        
        elif cmd in ['/stats', '/s']:
            self.show_stats()
        
        elif cmd in ['/monitor', '/m']:
            self.toggle_monitor_display()
        
        elif cmd in ['/backends', '/b']:
            self.show_backends()
        
        elif cmd in ['/system', '/sys']:
            self.show_system_info()
        
        else:
            print(f"{Fore.RED}æœªçŸ¥å‘½ä»¤: {command}{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}è¾“å…¥ /help æŸ¥çœ‹å¯ç”¨å‘½ä»¤{Style.RESET_ALL}")
        
        return False
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        help_text = f"""
{Fore.CYAN}ğŸ’¡ å¯ç”¨å‘½ä»¤:{Style.RESET_ALL}
  {Fore.YELLOW}/help{Style.RESET_ALL}     - æ˜¾ç¤ºæ­¤å¸®åŠ©
  {Fore.YELLOW}/clear{Style.RESET_ALL}    - æ¸…ç©ºèŠå¤©å†å²å’Œå±å¹•
  {Fore.YELLOW}/stats{Style.RESET_ALL}    - æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
  {Fore.YELLOW}/monitor{Style.RESET_ALL}  - åˆ‡æ¢æ€§èƒ½ç›‘æ§æ˜¾ç¤º
  {Fore.YELLOW}/backends{Style.RESET_ALL} - æ˜¾ç¤ºç¡¬ä»¶åç«¯ä¿¡æ¯
  {Fore.YELLOW}/system{Style.RESET_ALL}   - æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
  {Fore.YELLOW}/quit{Style.RESET_ALL}     - é€€å‡ºç¨‹åº
"""
        print(help_text)
    
    def show_stats(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        if not self.llm:
            print(f"{Fore.RED}æ¨¡å‹æœªåŠ è½½{Style.RESET_ALL}")
            return
        
        stats = self.llm.get_stats()
        print(f"\n{Fore.CYAN}ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:{Style.RESET_ALL}")
        for key, value in stats.items():
            print(f"  {key}: {value}")
    
    def toggle_monitor_display(self):
        """åˆ‡æ¢æ€§èƒ½ç›‘æ§æ˜¾ç¤º"""
        if not self.monitor:
            print(f"{Fore.RED}æ€§èƒ½ç›‘æ§æœªå¯ç”¨{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}è¯·ä½¿ç”¨ --monitor å‚æ•°å¯åŠ¨ç¨‹åº{Style.RESET_ALL}")
            return
        
        self.monitor_display_active = not self.monitor_display_active
        if self.monitor_display_active:
            print(self.monitor.format_metrics())
            print(f"{Fore.GREEN}ğŸ–¥ï¸ æ€§èƒ½ç›‘æ§æ˜¾ç¤ºå·²å¼€å¯{Style.RESET_ALL}")
        else:
            print(f"{Fore.YELLOW}ğŸ–¥ï¸ æ€§èƒ½ç›‘æ§æ˜¾ç¤ºå·²å…³é—­{Style.RESET_ALL}")
    
    def show_backends(self):
        """æ˜¾ç¤ºç¡¬ä»¶åç«¯ä¿¡æ¯"""
        if not hasattr(self.llm, 'available_backends'):
            print(f"{Fore.RED}æ— æ³•è·å–åç«¯ä¿¡æ¯{Style.RESET_ALL}")
            return
        
        backends = self.llm.available_backends
        current_backend = getattr(self.llm, 'selected_backend', 'unknown')
        
        print(f"\n{Fore.CYAN}ğŸ–¥ï¸ ç¡¬ä»¶åç«¯ä¿¡æ¯:{Style.RESET_ALL}")
        print(f"{Fore.GREEN}å½“å‰ä½¿ç”¨: {current_backend.upper()}{Style.RESET_ALL}")
        print(f"\nå¯ç”¨åç«¯:")
        
        for backend, available in backends.items():
            status = f"{Fore.GREEN}âœ“{Style.RESET_ALL}" if available else f"{Fore.RED}âœ—{Style.RESET_ALL}"
            current = f" {Fore.YELLOW}(å½“å‰){Style.RESET_ALL}" if backend == current_backend else ""
            config = HardwareDetector.get_optimal_config(backend)
            print(f"  {status} {backend.upper()}{current}: {config}")
    
    def show_system_info(self):
        """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
        HardwareDetector.print_system_info()
    
    def _clear_screen(self):
        """æ¸…å±"""
        os.system('cls' if os.name == 'nt' else 'clear')


class SimplePromptInterface:
    """ç®€å•çš„å•æ¬¡é—®ç­”ç•Œé¢"""
    
    @staticmethod
    def run_single_prompt(model_path: str, prompt: str, backend: str = 'auto', 
                         monitor: bool = False, **config):
        """è¿è¡Œå•æ¬¡é—®ç­”"""
        print(f"{Fore.CYAN}ğŸ¤– å•æ¬¡é—®ç­”æ¨¡å¼{Style.RESET_ALL}")
        
        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§
        performance_monitor = PerformanceMonitor() if monitor else None
        if performance_monitor:
            performance_monitor.start_monitoring()
        
        # åˆå§‹åŒ–æ¨¡å‹
        llm = SimpleLLM(model_path, backend=backend, monitor=performance_monitor, **config)
        
        if llm.load_model():
            print(f"\n{Fore.BLUE}é—®é¢˜: {prompt}{Style.RESET_ALL}")
            print(f"{Fore.GREEN}å›ç­”: {Style.RESET_ALL}", end="", flush=True)
            
            for token in llm.generate_response(prompt):
                if not token.startswith('\nğŸ“Š'):
                    print(token, end="", flush=True)
            print()
            
            # æ˜¾ç¤ºæ€§èƒ½ç›‘æ§ç»“æœ
            if performance_monitor:
                print("\n" + performance_monitor.format_metrics())
                performance_monitor.stop_monitoring()
        
        return True